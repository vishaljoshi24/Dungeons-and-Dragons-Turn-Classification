{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOHPB39PiBmEahDyv2p8tP2",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/vishaljoshi24/DnD-AutoPrompt/blob/main/classifying_turns.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FQIDU2Cz0wpC"
      },
      "outputs": [],
      "source": [
        "!pip install -U dspy"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SET-UP"
      ],
      "metadata": {
        "id": "VA6OafSA8xPk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import dspy\n",
        "\n",
        "lm = dspy.LM(\"ollama_chat/llama3.2:1b\", api_base = \"http://localhost:11434\", api_key=\"b229890ea0664f6193770b4b470f3e74.nJRqYf9TBF9MX24r4fUQQqqB\")\n",
        "dspy.configure(lm=lm)"
      ],
      "metadata": {
        "id": "VKb_un__1-u1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "DATA"
      ],
      "metadata": {
        "id": "v61m3_Is3K45"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "cx54UKtT3Mn4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = load_dataset(\"garrykuwanto/crd3_training_pairs\", download_mode=\"force_redownload\")"
      ],
      "metadata": {
        "id": "R20b_unY3mtr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.DataFrame(dataset)"
      ],
      "metadata": {
        "id": "PRStCyWF32xX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "query = []\n",
        "passage = []\n",
        "for i in range(len(df.train[0:])):\n",
        "  query.append(df.train[i]['query'])\n",
        "  passage.append(df.train[i]['passage'])\n",
        "\n",
        "df1 = pd.DataFrame({'context': query, 'current turn': passage})"
      ],
      "metadata": {
        "id": "Bpz_roNxa0eO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1"
      ],
      "metadata": {
        "id": "hiOEXQjklbiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2 = df1[500:510]\n",
        "\n",
        "df2"
      ],
      "metadata": {
        "id": "O3uS-yqANw11"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df2.insert(2, 'quality', ['goal-oriented', 'DM turn', 'goal-oriented', 'DM turn', 'collaborative action', 'open ended', 'contextually relevant', 'collaborative', 'goal-oriented', 'open ended'  ])"
      ],
      "metadata": {
        "id": "A2BZQ-aDN0k6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "examples = []\n",
        "\n",
        "for context, turn, quality in df2.values:\n",
        "  examples.append(dspy.Example(context=context, turn=turn, quality=quality).with_inputs(\"context\", \"current turn\", \"quality\"))\n",
        "\n",
        "examples"
      ],
      "metadata": {
        "id": "oiMzC7r5uzEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SIGNATURES"
      ],
      "metadata": {
        "id": "QE_KiE4-8t3n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Literal\n",
        "\n",
        "\n",
        "from typing import Literal\n",
        "\n",
        "class TurnClassifier(dspy.Signature):\n",
        "    data: dict[str, str] = dspy.InputField(desc = \"\"\"\n",
        "  {'context': 'The three previous game turns which describe a player's action or their dialogue.',\n",
        "   'turn': 'The current turn taken by a player, which can include a description of an action or a piece of dialogue.',\n",
        "   }\"\"\")\n",
        "    behaviour_class: Literal['collaborative',\n",
        "                             'contextually-relevant',\n",
        "                             'goal-oriented',\n",
        "                             'open-ended'] = dspy.OutputField(desc=\"The feature of a player's turn within a Dungeons & Dragons game: collaborative, open-ended, goal-oriented, contextually relevant\")\n",
        "\n",
        "classify = dspy.ChainOfThought(BehaviourClassifier)\n",
        "\n",
        "# class PlayerInstruction(dspy.Signature):\n",
        "#   \"Generate a prompt which instructs an agent how to behave as a D&D player based on the labelled quality for a specific gameplay turn\"\n",
        "\n",
        "#   examples: dict[str, str] = dspy.InputField(desc = \"\"\"\n",
        "#   {'context': 'The three previous game turns which describe a player's action or their dialogue.',\n",
        "#    'turn': 'The current turn taken by a player, which can include a description of an action or a piece of dialogue.',\n",
        "#    'behaviour': 'The behavious exhibited by a player in their turn within a Dungeons & Dragons game: open ended, goal oriented, contextually relevant and collaborative. DM turns should be ignored.'\n",
        "#   }\n",
        "#   \"\"\")\n",
        "#   instruction: str = dspy.OutputField(desc = \"instruction on how to behave within a D&D game which describes the quality label of a particular turn.\")\n",
        "\n",
        "  # context: dict[str, str] = dspy.InputField(desc = \"The three previous game turns which describe a player's action or their dialogue\")\n",
        "  # turn: dict[str,str] = dspy.InputField(desc = \"The current turn taken by a player, which can include a description of an action or a piece of dialogue.\")\n",
        "  # quality: dict[str,str] = dspy.InputField(desc=\"The quality of a player's actions within a Dungeons & Dragons game: open-ended, goal-oriented, contextually relevant, collaborative.\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "o5QyxEWiUaYF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "MODULES"
      ],
      "metadata": {
        "id": "Lzwnar4vSrV3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class PromptGenerator(dspy.Module):\n",
        "#   def __init__(self):\n",
        "#     # self.classifier = dspy.ChainOfThought(Quality, caching = False)\n",
        "#     self.generator = dspy.ChainOfThought(PlayerInstruction, caching = False)\n",
        "\n",
        "#   def forward(self, examples, **kwargs):\n",
        "#     # quality_class = self.classifier(action=action).quality\n",
        "#     prompt = self.generator(examples=examples).instruction\n",
        "#     return prompt"
      ],
      "metadata": {
        "id": "QhhIiFORMnuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# generate_prompts = PromptGenerator()\n",
        "# prompt_list = []\n",
        "# for i in range(len(examples)):\n",
        "#   prompt_list.append(generate_prompts(i))\n",
        "# prompt_list"
      ],
      "metadata": {
        "id": "u_Ipd8qVyiVi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVALUATION QUESTIONS"
      ],
      "metadata": {
        "id": "Yk-eabmWS4cA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "  clear = \"Is the assessed prompt specific, unambiguous and easy to interpret?\"\n",
        "\n",
        "  relevant = \"Does the prompt align with the categorised quality of D&D player behaviour that it is trying to express?\"\n",
        "\n",
        "  correct = \"Is the assessed prompt correct with respect to the player quality that it is describing?\"\n",
        "\n",
        "  complete = \"Does the prompt fully express the quality of the player's behaviour, taking into account additional context or secondary queries that might have been implied or included in the prompt?\"\n",
        "\n"
      ],
      "metadata": {
        "id": "omS-zBIVS6A5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "EVALUATION SIGNATURES"
      ],
      "metadata": {
        "id": "R0JwIa31_iYJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class Assess(dspy.Signature):\n",
        "#   \"\"\"Assess the quality of a prompt along a specified dimension.\"\"\"\n",
        "\n",
        "#   assessed_text: list[str] = dspy.InputField()\n",
        "#   assessment_question: str = dspy.InputField()\n",
        "#   assessment_answer: bool = dspy.OutputField()"
      ],
      "metadata": {
        "id": "je00dWh7_l3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clarity_evaluation = dspy.ChainOfThought(Assess, caching=False)\n",
        "# relevance_evaluation = dspy.ChainOfThought(Assess, caching=False)\n",
        "# correctness_evaluation = dspy.ChainOfThought(Assess, caching=False)\n",
        "# completeness_evaluation = dspy.ChainOfThought(Assess, caching=False)"
      ],
      "metadata": {
        "id": "2dvdzKhTAAS9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# clarity_list = []\n",
        "# for i in prompt_list:\n",
        "#   prompt_clarity = clarity_evaluation(assessed_text=i, assessment_question=\"Is the assessed prompt specific, unambiguous and easy to interpret?\")\n",
        "#   clarity_list.append(prompt_clarity)\n",
        "\n",
        "# clarity_list\n",
        "\n",
        "# # clarity_evaluation(assessed_text=clarity_list, assessment_question=\"Is the assessed prompt specific, unambiguous and easy to interpret?\")"
      ],
      "metadata": {
        "id": "lr0VJCXPAabk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# relevance_list = []\n",
        "# for i in prompt_list:\n",
        "#   prompt_relevance = relevance_evaluation(assessed_text=i, assessment_question=\"Does the prompt align with the categorised quality of D&D player behaviour that it is trying to express?\")\n",
        "#   relevance_list.append(prompt_relevance)\n",
        "\n",
        "# relevance_list\n",
        "\n",
        "# # relevance_evaluation(assessed_text=example_1, assessment_question=\"Does the prompt align with the categorised quality of D&D player behaviour that it is trying to express?\")"
      ],
      "metadata": {
        "id": "MBv7g9_hEOSH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# correctness_list = []\n",
        "# for i in prompt_list:\n",
        "#   prompt_correctness = correctness_evaluation(assessed_text=i, assessment_question=\"Is the assessed prompt correct with respect to the player quality that it is describing?\")\n",
        "#   correctness_list.append(prompt_correctness)\n",
        "\n",
        "# correctness_list\n",
        "\n",
        "# # correctness_evaluation(assessed_text=example_1, assessment_question=\"Is the assessed prompt correct with respect to the player quality that it is describing?\")"
      ],
      "metadata": {
        "id": "9k_xaQYeA3zG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# completeness_list = []\n",
        "# for i in prompt_list:\n",
        "#   prompt_completeness = completeness_evaluation(assessed_text=i, assessment_question=\"Does the prompt fully express the quality of the player's behaviour, taking into account additional context or secondary queries that might have been implied or included in the prompt?\")\n",
        "#   completeness_list.append(prompt_completeness)\n",
        "\n",
        "# completeness_list\n",
        "\n",
        "# # completeness_evaluation(assessed_text=example_1, assessment_question=\"Does the prompt fully express the quality of the player's behaviour, taking into account additional context or secondary queries that might have been implied or included in the prompt?\")"
      ],
      "metadata": {
        "id": "-Atn-cR7EqYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# class ScorePrompt(dspy.Module):\n",
        "#   def __init__(self):\n",
        "#     self.assessment = dspy.ChainOfThought(Assess, caching=False)\n",
        "\n",
        "#   def forward(self, prompt_list, **kwargs):\n",
        "#     score_list = []\n",
        "#     for i in prompt_list:\n",
        "#       assess_clarity = self.assessment(assessed_text=i, assessment_question=\"Is the assessed prompt specific, unambiguous and easy to interpret?\").assessment_answer\n",
        "#       assess_relevance = self.assessment(assessed_text=i, assessment_question=\"Does the prompt align with the categorised quality of D&D player behaviour that it is trying to express?\").assessment_answer\n",
        "#       assess_correctness = self.assessment(assessed_text=i, assessment_question=\"Is the assessed prompt correct with respect to the player quality that it is describing?\").assessment_answer\n",
        "#       assess_completeness = self.assessment(assessed_text=i, assessment_question=\"Does the prompt fully express the quality of the player's behaviour, taking into account additional context or secondary queries that might have been implied or included in the prompt?\").assessment_answer\n",
        "\n",
        "#       if assess_clarity == True:\n",
        "#         assess_clarity = 1\n",
        "#       else:\n",
        "#         assess_clarity = 0\n",
        "#       if assess_relevance == True:\n",
        "#         assess_relevance = 1\n",
        "#       else:\n",
        "#         assess_relevance = 0\n",
        "#       if assess_correctness == True:\n",
        "#         assess_correctness = 1\n",
        "#       else:\n",
        "#         assess_correctness = 0\n",
        "#       if assess_completeness == True:\n",
        "#         assess_completeness = 1\n",
        "#       else:\n",
        "#         assess_completeness = 0\n",
        "\n",
        "#       score = (assess_clarity + assess_relevance + assess_correctness + assess_completeness)/4\n",
        "#       score_list.append(score)\n",
        "\n",
        "#     return score_list"
      ],
      "metadata": {
        "id": "K6lacPm4Ft0n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt_score_list = ScorePrompt()\n",
        "# prompt_score_list(prompt_list=prompt_list)"
      ],
      "metadata": {
        "id": "CjpDgLYnGOjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Eds22XDPXWsV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}